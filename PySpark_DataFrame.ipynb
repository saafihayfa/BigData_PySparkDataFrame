{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "en pyspark il faut commencer par un sparksession.\n",
    "Le point d'entrée pour programmer Spark avec l'API Dataset et DataFrame.\n",
    "Une SparkSession peut être utilisée pour créer DataFrame, s'enregistrer en DataFrametant que tables, exécuter du SQL sur \n",
    "des tables, mettre en cache des tables et lire des fichiers parquet\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('DataFrame').getOrCreate()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-OSLIV4D:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21545d969d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lire un dataset \n",
    "#inferSchema = définir les types de données        (rq: au bebut tout les données sont de types String par defaut)\n",
    "df_pyspark=spark.read.option('header','true').csv('data.csv',inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- salaire;: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#vérification des types de données des colonnes\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+--------+\n",
      "|Name| Age|Experience|salaire;|\n",
      "+----+----+----------+--------+\n",
      "| aaa|  11|         1|    100;|\n",
      "| bbb|  22|         2|    200;|\n",
      "| ccc|  33|         3|    300;|\n",
      "| ddd|  44|         4|    400;|\n",
      "| eee|  55|         5|    500;|\n",
      "| fff|  66|         6|    600;|\n",
      "| ggg|null|      null|    700;|\n",
      "|null|  88|         8|    800;|\n",
      "|null|  99|      null|    null|\n",
      "+----+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()          #visualiser le dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type de dataset : DataFrame c'est une structure de données qu'on applique sur elle différentes types d'opérations \n",
    "type (df_pyspark)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='aaa', Age=11, Experience=1, salaire;='100;'),\n",
       " Row(Name='bbb', Age=22, Experience=2, salaire;='200;'),\n",
       " Row(Name='ccc', Age=33, Experience=3, salaire;='300;'),\n",
       " Row(Name='ddd', Age=44, Experience=4, salaire;='400;'),\n",
       " Row(Name='eee', Age=55, Experience=5, salaire;='500;'),\n",
       " Row(Name='fff', Age=66, Experience=6, salaire;='600;'),\n",
       " Row(Name='ggg', Age=None, Experience=None, salaire;='700;'),\n",
       " Row(Name=None, Age=88, Experience=8, salaire;='800;'),\n",
       " Row(Name=None, Age=99, Experience=None, salaire;=None)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(9)   #les lignes de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'salaire;']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#les colonnes de dataset\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Experience', 'int'),\n",
       " ('salaire;', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vérifier les types de données\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|Experience|\n",
      "+----+----------+\n",
      "| aaa|         1|\n",
      "| bbb|         2|\n",
      "| ccc|         3|\n",
      "| ddd|         4|\n",
      "| eee|         5|\n",
      "| fff|         6|\n",
      "| ggg|      null|\n",
      "|null|         8|\n",
      "|null|      null|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sélection de colonnes (Name et Experience )et indexation\n",
    "select=df_pyspark.select(['Name','Experience'])\n",
    "select.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+--------+---------------------+\n",
      "|Name| Age|Experience|salaire;|Experience aprés 2ans|\n",
      "+----+----+----------+--------+---------------------+\n",
      "| aaa|  11|         1|    100;|                    3|\n",
      "| bbb|  22|         2|    200;|                    4|\n",
      "| ccc|  33|         3|    300;|                    5|\n",
      "| ddd|  44|         4|    400;|                    6|\n",
      "| eee|  55|         5|    500;|                    7|\n",
      "| fff|  66|         6|    600;|                    8|\n",
      "| ggg|null|      null|    700;|                 null|\n",
      "|null|  88|         8|    800;|                   10|\n",
      "|null|  99|      null|    null|                 null|\n",
      "+----+----+----------+--------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ajout de colonnes en dataframe\n",
    "ajout=df_pyspark.withColumn('Experience aprés 2ans',df_pyspark['Experience']+2)\n",
    "ajout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+--------+\n",
      "|Name| Age|Experience|salaire;|\n",
      "+----+----+----------+--------+\n",
      "| aaa|  11|         1|    100;|\n",
      "| bbb|  22|         2|    200;|\n",
      "| ccc|  33|         3|    300;|\n",
      "| ddd|  44|         4|    400;|\n",
      "| eee|  55|         5|    500;|\n",
      "| fff|  66|         6|    600;|\n",
      "| ggg|null|      null|    700;|\n",
      "|null|  88|         8|    800;|\n",
      "|null|  99|      null|    null|\n",
      "+----+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Supprimer de colonnes en dataframe\n",
    "supp=df_pyspark.drop('Experience aprés 2ans')\n",
    "supp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+--------+\n",
      "|New Name| Age|Experience|salaire;|\n",
      "+--------+----+----------+--------+\n",
      "|     aaa|  11|         1|    100;|\n",
      "|     bbb|  22|         2|    200;|\n",
      "|     ccc|  33|         3|    300;|\n",
      "|     ddd|  44|         4|    400;|\n",
      "|     eee|  55|         5|    500;|\n",
      "|     fff|  66|         6|    600;|\n",
      "|     ggg|null|      null|    700;|\n",
      "|    null|  88|         8|    800;|\n",
      "|    null|  99|      null|    null|\n",
      "+--------+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#renommer les colonnes\n",
    "ren=df_pyspark.withColumnRenamed('Name','New Name')\n",
    "ren.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+--------+\n",
      "|Name|Age|Experience|salaire;|\n",
      "+----+---+----------+--------+\n",
      "| aaa| 11|         1|    100;|\n",
      "| bbb| 22|         2|    200;|\n",
      "| ccc| 33|         3|    300;|\n",
      "| ddd| 44|         4|    400;|\n",
      "| eee| 55|         5|    500;|\n",
      "| fff| 66|         6|    600;|\n",
      "+----+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gestion des valeurs manquantes\n",
    "#supprimer les lignes qui ont des valeurs null\n",
    "sup_nul=df_pyspark.na.drop()\n",
    "sup_nul.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+--------+\n",
      "|Name| Age|Experience|salaire;|\n",
      "+----+----+----------+--------+\n",
      "| aaa|  11|         1|    100;|\n",
      "| bbb|  22|         2|    200;|\n",
      "| ccc|  33|         3|    300;|\n",
      "| ddd|  44|         4|    400;|\n",
      "| eee|  55|         5|    500;|\n",
      "| fff|  66|         6|    600;|\n",
      "| ggg|null|      null|    700;|\n",
      "|null|  88|         8|    800;|\n",
      "|null|  99|      null|    null|\n",
      "+----+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how=all : supprimer les lignes dont toutes les valeurs sont nulls \n",
    "df_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+--------+\n",
      "|Name|Age|Experience|salaire;|\n",
      "+----+---+----------+--------+\n",
      "| aaa| 11|         1|    100;|\n",
      "| bbb| 22|         2|    200;|\n",
      "| ccc| 33|         3|    300;|\n",
      "| ddd| 44|         4|    400;|\n",
      "| eee| 55|         5|    500;|\n",
      "| fff| 66|         6|    600;|\n",
      "+----+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how=any : supprimer les lignes qui contient au moins une valeur null\n",
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+--------+\n",
      "|Name|Age|Experience|salaire;|\n",
      "+----+---+----------+--------+\n",
      "| aaa| 11|         1|    100;|\n",
      "| bbb| 22|         2|    200;|\n",
      "| ccc| 33|         3|    300;|\n",
      "| ddd| 44|         4|    400;|\n",
      "| eee| 55|         5|    500;|\n",
      "| fff| 66|         6|    600;|\n",
      "|null| 88|         8|    800;|\n",
      "+----+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#threshold : supprimer les lignes qui ne contient pas au moins 3 valeurs non nulls\n",
    "df_pyspark.na.drop(how=\"any\",thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+--------+\n",
      "|Name| Age|Experience|salaire;|\n",
      "+----+----+----------+--------+\n",
      "| aaa|  11|         1|    100;|\n",
      "| bbb|  22|         2|    200;|\n",
      "| ccc|  33|         3|    300;|\n",
      "| ddd|  44|         4|    400;|\n",
      "| eee|  55|         5|    500;|\n",
      "| fff|  66|         6|    600;|\n",
      "| ggg|null|      null|    700;|\n",
      "+----+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Subset : supprimer les lignes qui contiennent des valeurs nulls dans une colonne donnée\n",
    "df_pyspark.na.drop(how=\"any\",subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+----------+----------------+\n",
      "|            Name| Age|Experience|        salaire;|\n",
      "+----------------+----+----------+----------------+\n",
      "|             aaa|  11|         1|            100;|\n",
      "|             bbb|  22|         2|            200;|\n",
      "|             ccc|  33|         3|            300;|\n",
      "|             ddd|  44|         4|            400;|\n",
      "|             eee|  55|         5|            500;|\n",
      "|             fff|  66|         6|            600;|\n",
      "|             ggg|null|      null|            700;|\n",
      "|valeur manquante|  88|         8|            800;|\n",
      "|valeur manquante|  99|      null|valeur manquante|\n",
      "+----------------+----+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fill : remplir les valeurs manquantes\n",
    "df_pyspark.na.fill('valeur manquante').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+----------+--------+\n",
      "|            Name| Age|Experience|salaire;|\n",
      "+----------------+----+----------+--------+\n",
      "|             aaa|  11|         1|    100;|\n",
      "|             bbb|  22|         2|    200;|\n",
      "|             ccc|  33|         3|    300;|\n",
      "|             ddd|  44|         4|    400;|\n",
      "|             eee|  55|         5|    500;|\n",
      "|             fff|  66|         6|    600;|\n",
      "|             ggg|null|      null|    700;|\n",
      "|valeur manquante|  88|         8|    800;|\n",
      "|valeur manquante|  99|      null|    null|\n",
      "+----------------+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fill : remplir les valeurs manquantes de colonne (Name)\n",
    "df_pyspark.na.fill('valeur manquante','Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+--------+\n",
      "|Name|Age|Experience|salaire;|\n",
      "+----+---+----------+--------+\n",
      "| aaa| 11|         1|    100;|\n",
      "| bbb| 22|         2|    200;|\n",
      "| ccc| 33|         3|    300;|\n",
      "| ddd| 44|         4|    400;|\n",
      "+----+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filtrage des données \n",
    "#les personnes dont leurs ages inférieur ou egal à 44 \n",
    "df_pyspark.filter(\"Age<=44\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+--------+\n",
      "|Name|Age|Experience|salaire;|\n",
      "+----+---+----------+--------+\n",
      "| eee| 55|         5|    500;|\n",
      "| fff| 66|         6|    600;|\n",
      "|null| 88|         8|    800;|\n",
      "|null| 99|      null|    null|\n",
      "+----+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#les personnes dont leurs ages supérieur à 44            (la négation (~))\n",
    "df_pyspark.filter(~(df_pyspark['Age']<=44)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|Experience|\n",
      "+----+----------+\n",
      "| aaa|         1|\n",
      "| bbb|         2|\n",
      "| ccc|         3|\n",
      "| ddd|         4|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selection les noms et l'expérience des personnes agé moins de 44 ans \n",
    "df_pyspark.filter(\"Age<=44\").select('Name','Experience').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+--------+\n",
      "|Name|Age|Experience|salaire;|\n",
      "+----+---+----------+--------+\n",
      "| bbb| 22|         2|    200;|\n",
      "| ccc| 33|         3|    300;|\n",
      "| ddd| 44|         4|    400;|\n",
      "+----+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selection les personnes qui ont une expérience entre 2 et 4 ans (multiple condition)\n",
    "df_pyspark.filter((df_pyspark['Experience']>=2) &\n",
    "                 (df_pyspark['Experience']<=4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
